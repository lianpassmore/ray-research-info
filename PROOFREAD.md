# RAY Participant Resource Guide — Full Proofreading Document

> All visible text from the website, in reading order, page by page.
> Browser tab titles shown in `[brackets]`.

---

## Global Elements

### Navigation Bar
- **Home link:** "← Home"
- **Nav items:** About | Safety | Research | Privacy

### Contact Footer (appears on every page)

**Researcher**
Lian Passmore
lianpassmore@gmail.com
027 566 8803

**Supervisor**
Felix Scholz, AcademyEX
mtf@academyex.com

Master of Technological Futures Candidate
Project RISE: Relational Insights for Systemic Empowerment

### Meta Description
RAY Research Pilot - Participant Resource Guide. February 12-26, 2026.

---

## Page 1: Home

`[RAY - Participant Resource Guide]`

**Research Pilot**

# RAY

## Participant Resource Guide

February 12-26, 2026

---

### Navigation Cards

**Safety**
Crisis resources, boundaries & support

**About Ray**
What Ray is, FAQ & how it works

**The Research**
Cultural foundations, research questions & what happens next

**Privacy & Timeline**
Your data, your rights & key dates

---

### Why Your Voice Matters

Most AI is designed without input from the people who'll actually use it for vulnerable conversations. You're helping design AI that centres relationality, dignity, and care — not just efficiency and verification.

Your feedback will directly influence:
- Whether this approach to AI design actually works
- What ethical frameworks are needed for conversational AI in vulnerable contexts
- How to build AI that respects Maori and Pasifika values authentically

---

### Closing Message

**Nga mihi nui for participating in this mahi.**

Your voice matters, and the researcher is grateful for the opportunity to walk alongside you in this research.

---

## Page 2: Safety

`[Safety - RAY]`

# Safety Protocols

Because Ray deals with vulnerable conversations, safety protocols are built in.

---

### Crisis Detection System

If Ray detects signs of serious distress — suicidal thoughts, self-harm, thoughts of harming others, abuse disclosures, or severe mental health crisis — three things happen:

1. **Ray stops the coaching conversation immediately**
2. **Ray provides crisis resources directly**
3. **Ray alerts the researcher**, who will follow up as soon as possible

**Important:** The researcher cannot guarantee a response within a specific timeframe. If you're using Ray late at night, the response will come the next morning. The crisis resources below are available 24/7 for immediate support.

---

### Important Boundaries

- Ray is **coaching, NOT therapy**
- Ray **cannot treat mental health conditions**
- If you need therapy, Ray will help you recognise that and refer you
- Crisis protocols are a **safety net, not a replacement** for professional help

---

### Crisis Resources (Always Available)

These resources are available 24/7 and are always visible in the app.

**Mental Health Support**
- **1737** Call or Text (24/7)
- **Lifeline** 0800 543 354

**Domestic Violence**
- **Women's Refuge** 0800 733 843

**Emergency**
- **Emergency** 111

---

### Questions or Concerns?

If you have any questions about the pilot, Ray, or the research, please reach out anytime.

If something doesn't feel right, or if Ray's responses concern you, contact the researcher immediately.

**Lian Passmore**
lianpassmore@gmail.com
027 566 8803

**Page navigation:** ← Home | About Ray →

---

## Page 3: About Ray

`[About Ray - RAY]`

# About Ray

Ray is an AI relationship coach built as part of a Master's research project exploring whether AI can be designed ethically for vulnerable conversations.

Ray works with any relationship: romantic, family, friends, work, yourself. It helps you see patterns you might be stuck in.

---

### What Ray Is
- AI coaching that helps you recognise patterns and make decisions
- Voice-based (with text option)
- Transparent about being AI — never pretends to be human
- Available for any relationship type
- Can work with two people at once (you and a partner, friend, family member)

### What Ray Isn't
- Therapy or mental health treatment
- Crisis intervention
- A replacement for professional support
- Perfect or always right

---

### Frequently Asked Questions

**What is Ray exactly?**
Ray is a conversational AI relationship coach. You talk (or type), Ray responds. It's designed to help you think through what's going on in your relationships — patterns you might not see on your own, dynamics that keep showing up, or decisions you're wrestling with.

**Is Ray a therapist?**
No. Ray is coaching, not therapy. Ray can't treat mental health conditions or provide crisis intervention. If you need professional support, Ray will help you recognise that and point you to the right place.

**Can I use Ray with someone else?**
Yes. You can have a session alone, or with a partner, friend, or family member present. Ray can work with both of you at once.

**Does Ray remember previous conversations?**
No. For this pilot, each session starts fresh. Ray has no memory of what you've talked about before. Part of the research is learning whether that fresh-start approach feels helpful or limiting.

**How long are sessions?**
Sessions are capped at 1 hour, but you decide how long you engage. Most sessions run 15-45 minutes. Use Ray as often as you like during the pilot.

**What if I'm in crisis?**
Ray can't help in a crisis. If Ray detects signs of serious distress, it will:
1. Stop the coaching conversation immediately
2. Provide crisis resources directly
3. Alert the researcher

**Crisis resources are always available:**
- **1737** — Call or text, 24/7
- **Lifeline:** 0800 543 354
- **Emergency:** 111

**Important:** If you use Ray late at night and a crisis is detected, the researcher will be asleep and won't respond until morning. The resources above are available 24/7.

**Do I need to be tech-savvy?**
No. If you can have a phone conversation, you can use Ray. It's as simple as talking and listening.

**Can I use text instead of voice?**
Yes. Ray works via voice or text — your choice.

**What to expect from Ray**
Ray is transparent about being AI — it never pretends to be human. It's helpful for recognising patterns and making decisions, but it's not perfect or always right.

Ray might challenge you. That's intentional. The goal isn't to make you feel good — it's to help you see clearly.

**A note on connection**
Ray is designed to be a helpful thinking partner — like a mate on the back porch who'll listen while you talk things through. It's not a substitute for the real relationships in your life.

Some people find these kinds of conversations easy to lean into, and that's okay. Just know that Ray is a tool to support your thinking, not a relationship in itself.

---

### Why the Tui?

Ray's icon is a tui — a native New Zealand bird known for its bold, honest presence and distinctive voice.

**Direct and honest.**
Tui are confident, sometimes cheeky birds. They don't hide. Ray works the same way — it doesn't soften hard truths or tell you what you want to hear. It helps you see clearly.

**Distinctive voice.**
Tui are known for their complex, varied calls — layered, unmistakable, sometimes beautiful, sometimes strange. Ray is voice-first because vulnerable conversations need the texture and nuance that voice carries.

**Grounded in place.**
Tui are native to Aotearoa and culturally significant in Maori worldviews. They appear in whakatauki (proverbs) about truth-telling and clarity, and are associated with communication and voice. Ray is built on the same grounding — Maori and Pasifika values as foundation, not decoration.

**Beautiful but not precious.**
Tui are striking birds with their white throat tuft, but they're scrappy, common garden birds. You see them in parks, backyards, cities. They're not rare or distant. Ray is designed the same way — accessible and everyday, something you can turn to when you need it, not something you admire from afar.

**Wise but approachable.**
Tui are common enough to feel familiar, yet they carry mana. They're respected in Maori culture without being overly sacred or tapu. Ray holds that same balance — grounded in cultural values, designed to be used, not revered from a distance.

*The tui isn't just a logo. It's a signal about how this AI was built and what it's here to do.*

**Page navigation:** ← Safety | The Research →

---

## Page 4: The Research

`[Ray - The Research Info]`

# The Research

This Master's research asks: *can AI be designed ethically for vulnerable conversations?*

Relationships are one of the most vulnerable contexts we navigate. If AI can be built well for this, those principles can apply anywhere.

---

### What's Being Studied

The researcher is studying your *experience* of using Ray:
- Did it feel respectful, safe, and helpful?
- What worked and what didn't?
- How did the lack of memory between sessions feel?
- Did the cultural values come through in how Ray interacted with you?

**The researcher will NOT read your session transcripts.**
Transcripts are anonymised by AI (personal details removed) and assigned a participant number. Only high-level aggregate analysis is done — themes, topics, and patterns across all participants. What you talked about with Ray stays private.

---

### Research Questions

This pilot helps answer five questions for the final Master's project:

**1. Kei raro (Foundations)**
What barriers silence voices in current feedback systems?
*For Ray: Can conversational AI lower barriers that keep people from seeking relationship support?*

**2. Kei mua (Values)**
How can Maori and Pasifika values enhance system design?
*For Ray: Do the values of Va, Utu Tuturu, and Mana Motuhake actually show up in how Ray holds space?*

**3. Kei runga (Purpose)**
What outcomes matter to communities AND businesses?
*For Ray: Can AI coaching create meaningful outcomes while also being commercially viable?*

**4. Kei roto (Agency)**
How do we protect data sovereignty and emotional safety?
*For Ray: Does the data sovereignty approach work in practice? Do people feel emotionally safe?*

**5. Kei waho (Innovation)**
How can AI technology be developed ethically?
*For Ray: What does ethical development look like when AI is handling our most vulnerable conversations?*

Your experience using Ray directly answers these questions.

---

### Cultural Foundations

Ray is grounded in three core values from Maori and Pasifika worldviews:

**Va**
The sacred relational space between people.

**Utu Tuturu**
Enduring collective reciprocity.

**Mana Motuhake**
Absolute data sovereignty.

These aren't decorative concepts. They're built into how Ray thinks, responds, and holds space for vulnerable conversations.

---

### What Happens After the Pilot?

**This pilot is primarily a research case study for the final Master's project.** Your participation helps determine whether conversational AI can be designed ethically for vulnerable contexts.

**However — if Ray works well, it may become publicly available.**

That means:
- If Ray proves to be **safe** (crisis protocols work, no harm caused)
- If Ray proves to be **helpful** (people find it genuinely useful)
- If Ray proves to be **culturally grounded** (values aren't just decorative)
- If Ray passes all **safety and ethical audits**

...then Ray may launch as a subscription service in the App Store and Play Store so others can access it.

**To be clear:**
- Right now, this is research
- You're helping test whether this *should* exist
- Your feedback determines whether Ray ever becomes a public product
- If it does launch, your participation in this pilot won't obligate you to anything
- Any future version would honour the same data sovereignty principles

**Why this transparency matters:** This isn't just a report that gets shelved. If Ray is built well and proven safe, it's designed to help people navigate relationships with more clarity and confidence. Your voice in this pilot shapes whether that happens and how.

---

### Honouring the Space

Relationship conversations touch on things that sit close to the heart — emotions, identity, whanau, mana. For some people, this kind of korero naturally sits in a tapu-adjacent space. Using an AI tool for these conversations might feel unfamiliar or uncomfortable, and that's completely valid.

If it feels right for you, you're welcome to open or close your sessions in whatever way honours that space — whether that's a karakia, a moment of quiet reflection, a deep breath, or simply checking in with yourself before and after. There's no right or wrong way to do this. Ray is designed to hold space respectfully, and how you enter and leave that space is entirely yours.

**Page navigation:** ← About Ray | Privacy & Timeline →

---

## Page 5: Privacy & Timeline

`[Privacy & Timeline - RAY]`

# Privacy & Your Data

---

### What You Agreed To

When you consented to participate, you agreed to:
- Try Ray between February 12-26, 2026
- Give quick feedback after each session (~3 minutes)
- Complete a final reflection at the end (~15 minutes)
- Allow the researcher to analyse broad themes across all participants

You understood that:
- Your conversations are private
- Ray is coaching, not therapy
- Ray can't help in a crisis
- Sessions cap at 1 hour
- Each session starts fresh with no memory
- You can withdraw before March 1, 2026

---

### What Happens to Your Conversations

**The researcher will NOT read your session transcripts.**
Your conversations with Ray are private. The researcher does not read through what you talked about.

**So what happens to my transcripts?**
Transcripts are stored securely in ElevenLabs. Before any analysis, AI is used to strip out all personal details (names, places, identifying info). Each participant is then assigned a number — the researcher never sees your name attached to a transcript.

**Is my voice data classified as biometric data?**
Under some laws, your voice data may be classified as biometric information. ElevenLabs uses this data solely for verification and voice modelling — never for profiling or targeting.

**What kind of analysis is done?**
Only high-level aggregate analysis across all participants — broad themes, common topics, and patterns. The researcher is not reading or analysing your personal relationship content.

---

### Feedback & Reflections

**Your feedback and reflections are different from your session transcripts.**
The researcher **will** read your post-session feedback and final reflections, and will know who submitted them. This is so the researcher can understand your experience of using Ray.

**What if the researcher wants to quote me?**
If the researcher wants to use a quote from your reflections in the final findings, **they will check with you first**. Nothing is quoted without your permission, and all quotes are anonymised.

---

### What You Control

**Can I delete my data?**
Yes. You can delete individual sessions or your entire account anytime during the pilot.

**Can I withdraw?**
Yes. You can withdraw from the research completely before March 1, 2026. If you withdraw, all your data will be deleted and not used in the research.

**Can I opt out of AI training?**
Yes. You can prevent ElevenLabs from using your voice data to train their AI models. This doesn't affect Ray's research use.

**Is it anonymous?**
Yes. All data is anonymised (participant codes only, no names).

---

### How Your Data is Protected

- **Session transcripts** are anonymised by AI (personal details removed) and assigned a participant number — the researcher never reads them directly
- Only **high-level aggregate analysis** is done on transcripts (themes, topics, patterns)
- **Feedback and reflections** are read by the researcher, but any quotes require your permission first
- Transcripts stored securely in ElevenLabs' systems (US, Netherlands, Singapore) — all data transfers to the United States for storage
- **Voice data retention:** ElevenLabs stores voice recordings for up to 3 years after your last interaction, unless you request deletion sooner
- **Safety monitoring:** ElevenLabs may monitor conversations for safety and fraud prevention, and may share content with third-party moderators to maintain platform safety
- Research data is **permanently deleted** 2 years after the project ends

---

### ElevenLabs Platform & Data Handling

**Ray is built on ElevenLabs' voice technology. Here's what that means for your data.**
ElevenLabs processes your voice data under their Terms of Service and Privacy Policy, in addition to Ray's research protocols.

**What ElevenLabs does with your voice**
- Stores your voice recordings for up to **3 years** after your last session (unless you request deletion sooner)
- May classify your voice as **biometric data** under certain laws — used solely for verification and voice modelling, never for profiling or targeting
- Uses voice data to provide Ray's services and may use it to improve their AI models (**you can opt out** of training use)
- Monitors content for **safety and fraud prevention** — in certain cases, may share content with third-party moderators
- Stores data in the **US, Netherlands, and Singapore** (all data transfers to the United States for storage)

**Your ElevenLabs rights**
- Request deletion of your voice data anytime
- Opt out of AI training use
- Access, correct, or restrict processing of your data

**Two layers of data control:** The researcher controls how your conversation content is used for research purposes. ElevenLabs controls the technical processing and storage of your voice recordings. Ray's research data sovereignty protocols are separate from ElevenLabs' platform policies.

---

### What the Researcher Will Do With Your Insights

Your reflections about using Ray will help shape the final Master's project and answer whether conversational AI can be designed ethically for vulnerable interactions.

The researcher will:
- Run high-level aggregate analysis on anonymised transcripts (themes, topics, patterns across all participants)
- Read your feedback and reflections to understand your experience
- Share anonymised findings in the final project (April 2026 submission)
- Return findings to you — you'll receive a summary of what was learned
- Present at academic conferences and potentially publish in journals (always anonymised)
- Check with you before quoting any of your reflections

---

### Timeline

**Now**
You've already consented and created your account

**Feb 12, 2026**
Pilot begins — use Ray anytime

**Feb 12-26**
Two-week pilot period

**After each session**
Quick feedback (~3 minutes)

**By Feb 26, 2026**
Final reflection with Ray (~15 minutes)

**March 1, 2026**
Deadline to withdraw if you change your mind

**Page navigation:** ← The Research

---

## Notes

- **Browser tab titles:** Home = "RAY - Participant Resource Guide" | Safety = "Safety - RAY" | About = "About Ray - RAY" | Research = "Ray - The Research Info" | Privacy = "Privacy & Timeline - RAY"
- **Page flow:** Home → Safety → About Ray → The Research → Privacy & Timeline
- **Macrons/special characters in source code:** Tui (source has Tui with macron), whanau (source has whanau with macron), korero (source has korero with macron), whakatauki (source has whakatauki with macron), Va (source has Va with macron), karakia. Note: This markdown document may not render all macrons — verify against the live site.
